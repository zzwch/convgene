% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{JSD}
\alias{JSD}
\title{Jensen-Shannon Divergence}
\usage{
JSD(m, pseudocount = 1e-06, unit = log2)
}
\arguments{
\item{m}{matrix-like object, with a discrete distribution in each column}

\item{pseudocount}{m's elements less than zero will be forced to be zero, and m = m + pseudocount to avoid zero in the numerator and/or denominator.}

\item{unit}{log, log2, log10 used in calculation of Shannon entropy}
}
\value{
Jensen Shanonn Divergence
}
\description{
Jensenâ€“Shannon divergence is a method of measuring the similarity between two probability distributions.
}
\details{
for more, see
https://enterotype.embl.de/enterotypes.html
https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence
https://stackoverflow.com/questions/11226627/jensen-shannon-divergence-in-r
}
\examples{
# p & q are distributions so their elements should sum up to 1
p <- c(0.00029421, 0.42837957, 0.1371827, 0.00029419, 0.00029419,
       0.40526004, 0.02741252, 0.00029422, 0.00029417, 0.00029418)
q <- c(0.00476199, 0.004762, 0.004762, 0.00476202, 0.95714168,
       0.00476213, 0.00476212, 0.00476202, 0.00476202, 0.00476202)
JSD2(p,q)
H((p+q)/2)-(H(p) + H(q))/2
}
